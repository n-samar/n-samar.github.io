<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 6.13.3 (455969)"/><meta name="author" content="NIKOLA SAMARDZIC"/><meta name="created" content="2017-10-31 21:30:01 +0000"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2017-12-12 22:32:17 +0000"/><title>CS 111 Answers</title></head><body><ol start="8"><li>Possible issues with user-level implementations of threads that kernel-level thread implementation solves:</li><ol><li>What happens when a system call blocks the executing thread? <span style="font-style: italic;">i.e. </span>when a single thread blocks itself by making a system call, the system call will cause the whole process (along with all the other threads running in it) to block.</li><li>Exploiting multi-core processors. <span style="font-style: italic;">i.e.</span> the operating system can force kernel threads to run on different cores at the same time on multi-core processors. This is not possible from user space.</li></ol><li>Linux processes schedule the delivery of timer signals (SIGALARM) and register handlers for them. We can schedule a SIGALARM that will interrupt the thread if it runs too long.</li><li>There are two:</li><ol><li>The increase in throughput due to parallel execution</li><li>The increase of cache hits, due to reference locality between the different threads</li></ol><li>Segmentation allows specification of arbitrarily sized ranges of the address space that are valid and are used for a particular purpose, such as holding the process’ code or stack. Paging is used to divide allocated memory space into smaller pieces that allow the virtual memory management system to load, relocate, and otherwise manage the space more flexibly. Segmentation and paging might be used together to both specify the portions of the theoretical virtual memory space that a process is actually to use while allowing more flexible management of RAM use within those portions. Segmentation would specify the set of address ranges that are legally addressable, while paging would allow the OS to locate small sections of those address ranges in arbitrary page frames, or to move them off to disk, as necessary.</li><li>Possibility of nasty race conditions due to shared state; A single thread could crash a process</li><li>Parallelism, communication, lower resource consumption, responsiveness (possibility of no I/O blocking)</li><li>All thread stacks have to be on the same address space of the process they share. So, a thread can have a stack grow large enough to overwrite some other data segment or thread’s stack. This is solved by limiting a thread’s maximum size. This implies a thread’s stack should remain shallow throughout execution.</li><li>System calls</li><li>The following:</li><ol><li>Illusion of parallel execution of processes</li><li>Scheduling</li><li>The system call API</li><li>Machine virtualization</li></ol><li>Daemon threads are used to eject pages together in bigger sizes</li><li>Pages can be evicted from RAM onto disk in concrete sizes</li><li>The following:</li><ol><li>Transparency — i.e. the abstraction of physical memory</li><li>Efficiency</li><li>Protection</li><li>Illusion of infinite space</li><li>Illusion of contiguity of addresses</li><li>Illusion of no sharing of memory between processes</li></ol><li>The following:</li><ol><li>CPU Virtualization</li><li>Memory Virtualization</li><li>Concurrency</li><li>The System Call API</li><li>Persistence of storage</li></ol><li>The following:</li><ol><li>Resource virtualization</li><li>Dealing with concurrency</li><li>Persistency</li><li>Minimal overhead (in time, space, and energy consumption)</li><li>Protection and isolation</li><li>Reliability</li></ol><li>A process is a running program</li><li>The following:</li><ol><li>The contents and location of its memory</li><li>The contents of its registers</li><ol><li>The Program Counter</li><li>The Stack Pointer</li><li>The Frame Pointer</li></ol><li>List of open files/pipes</li></ol><li>Eager loading is slower and loads the entirety of the executable into RAM prior to execution. Lazy loading loads only as much data as is required to start running.</li><li>The following:</li><ol><li>Loading code and static data into memory</li><li>Allocating memory for the stack</li><li>Possibly allocate memory for the heap</li><li>Open STDERR, STDOUT, and STDIN file descriptors</li><li>Set Program Counter to beginning of main</li></ol><li>The following:</li><ol><li>Running</li><li>Ready</li><li>Blocked</li></ol><li>The following:</li><ol><li>register context — contains content of a stopped process's registers when it was interrupted</li><li>Start and size of process memory</li><li>Parent process ID</li><li>Status</li><li>Trap frame of current interrupt</li></ol><li>Performance (i.e. we want low OS overhead) and Control (i.e. we want the OS the always be in control of the processes)</li><li>Limited Direct Execution</li><li>By discriminating between user and kernel mode code and providing a system call API to user mode code that implements protected and dangerous operations in the CPU with adequate wrappers and error handling code.</li><li>Via a trap instruction. The trap instruction jumps into kernel mode and performs the required privileged operations.</li><li>It uses the trap table which points to adequate trap handlers which are kernel level code that handles traps. That is why each system call has a number associated with it to reference the right location to the trap table.</li><li>At boot time via a privileged instruction by the OS.</li><li>The following approaches have been used:</li><ol><li>Cooperative approach: Trust that each process will yield by itself when and if it runs too long by making a system call. Note that any system call will work, since all system calls transfer control to the OS, which can then - prior to running the system call code - evaluate if a process should keep running.</li><li>Non-cooperative approach: Uses timer interrupts as a mechanism. The timer interrupts periodically halt the running process and call on the pre-configured interrupt handler in the OS.</li></ol><li>This is usually implemented in hardware.</li><li>The user registers are saved to RAM upon every timer interrupt automatically through hardware. The kernel registers are only saved to RAM if the OS decides to perform a context switch and this is done through software.</li><li>The following:</li><ol><li>All processes run for the same amount of time</li><li>All processes are ready to run at the same point in time</li><li>Once started each job runs to completion</li><li>All jobs do no I/O. i.e. CPU bound</li><li>The runtime of each job is known</li></ol><li>The following:</li><ol><li>Fairness: Are all processes treated equally?</li><li>Turnaround time: The time from when a job was ready until it was completed</li><li>Response time: The time from when a job was ready until it got to execute some of its code</li></ol><li>The following:</li><ol><li>FIFO:</li><ol><li>Advantages: Simple and easy to implement;</li><li>Disadvantages: Possible long average and worst case wait time if processes run for variable lengths of time.</li></ol><li>Shortest Job First (SJF):</li><ol><li>Advantages: Simple; lowered waiting time in comparison to FIFO</li><li>Disadvantages: If jobs don’t arrive at the same time, a relatively long job could still start running before short jobs come into the ready queue. This produces the same effect as FIFO.</li></ol><li>Shortest Time-to-Completion First (STCF):</li><ol><li>Advantages: Preemptive; Same as shortest job but without the issue of bad order of processes entering ready queue.</li><li>Disadvantages: Bad response time (<span style="font-style: italic;">i.e.</span> the time from when a process is ready to when it is first scheduled to run some of its code)</li></ol><li>Round Robin (Run each program for only a specific time slice and then switch; battle between responsiveness and context switch overhead):</li><ol><li>Advantages: Great fairness and response time</li><li>Disadvantages: Bad turnaround time; Possibly too much overhead from context switches.</li></ol><li>Multi-Level Feedback Queue Scheduling (MLFQ)</li><ol><li>Advantages: Accounts for I/O bound processes, while having all advantages of Round Robin</li></ol></ol><li>When an I/O request is made by a process, that process cannot run any code until that request is fulfilled (assuming serialized execution). Therefore, any time that a process spends scheduled when it is actually just waiting for some I/O is wasted.</li><li>The Rules for MLFQ are: Insert each new job into the top most queue (has the shortest time slice); If the process requests I/O relatively frequently within its time slice, keep it in the top queue; if it mostly finishes its time slice, demote it down one queue; If a lower level queue process has a lot of I/O interrupt ends, promote it one level; if a process is lower down the queue and hasn’t run in a long time, promote it to avoid starvation. The MLFQ operates with the following goals:</li><ol><li>Give preference to short jobs</li><li>Give preference to I/O bound (interactive) processes</li><li>Separate processes into categories based on their need for the processor</li></ol><li>An method of computing in which multiple processes can run concurrently (this is not necessarily related to multi-core processors)</li><li>The following:</li><ol><li>Code segment — Stores the code for a process</li><li>The Stack — required for subroutines and local variables</li><li>The Heap — required for dynamic memory allocation</li><li>Data segment — used for global variables and static local variables</li></ol><li>The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. It uses best fit. Each block of memory has a predefined size and usually this is some power of two. The memory starts of as one big block. If a small memory allocation is required, the block starts to get recursively split in half (leaving one extra block (the buddy block) on each split) until a best fit for a power of two is produced and the allocation request is satisfied. When a block is freed, it is coalesced with its buddy block if the buddy block is free as well.</li><li>The sizeof() function is not a real function in C. It is defined at compile time and therefore treated as a reference to a value. Therefore if you write something like <span style="font-family: 'Andale Mono';">int x[10]; printf(“%d\n”, sizeof(x)); </span><span style="font-family: 'Helvetica Neue';">will return 40 (4 bytes is the sizeof int and there are 10 of them) because there is enough static information to figure this out. However, if you write </span><span style="font-family: 'Andale Mono';">int *x = malloc (10 * sizeof(int));  printf(“%d\n”, sizeof(x)); </span><span style="font-family: 'Helvetica Neue';">you will get the sizeof an integer pointer because the system cannot know at compile time what malloc is going to return to x.</span></li><li><span style="font-family: 'Helvetica Neue';">The Operating System keeps track of all the allocations and deallocation made by any </span><span style="font-family: 'Helvetica Neue';">process and is ultimately responsible to free all resources held by a process once that process exits. Memory leaks become a problem in user code if that code runs for a particularly long time.</span></li><li><span style="font-family: 'Helvetica Neue';">Address translation is the mechanism employed to translate virtual to physical addresses.</span></li><li><span style="font-family: 'Helvetica Neue';">Dynamic relocation is an mechanism that provides the functionality of segment relocation. It achieves this through using base and bounds registers.</span></li><li><span style="font-family: 'Helvetica Neue';">A free list is a linked list of all the free segments of physical memory.</span></li><li><span style="font-family: 'Helvetica Neue';">The processor status word is a bit that specifies if code is currently running in user or kernel mode.</span></li><li><span style="font-family: 'Helvetica Neue';">Dynamic relocation still assumes contiguity of segments and therefore suffers from external fragmentation. It also does not allow the current process to have memory requirements greater than available physical memory. Relocation is rather slow.</span></li><li><span style="font-family: 'Helvetica Neue';">The process will have a lot of unused memory (specifically between the stack and heap). The segments mostly use up all of their memory. So, having a base/bounds register pair is very beneficial to freeing up space and avoiding internal fragmentation.</span></li><li><span style="font-family: 'Helvetica Neue';">There are a couple of approaches:</span></li><ol><li><span style="font-family: 'Helvetica Neue';">Explicit approach — VAX/VMS used to use up the first two bits of each virtual address to specify the segment that address could be a part of.</span></li><li><span style="font-family: 'Helvetica Neue';">Implicit approach — Look at who is asking for the memory location. For example, if we are trying to peek at a location pointed to by a program counter, we know that that is in the code segment.</span></li></ol><li><span style="font-family: 'Helvetica Neue';">To support sharing of segments, every segment will have a first couple of bits reserved and used to specify which processes can access it (protection bits). Protection bits specify who gets read/write/execute permissions on a segment.</span></li><li>External fragmentation is an issue that arises when the sizes of contiguous free blocks are so small that they are practically useless. Internal fragmentation is an issue that arises when a process is reserved more space that it asked for.</li><li>Compaction is the process of relocating free blocks together and coalescing them together.</li><li>These are methods for implementing memory allocation (i.e. choosing the appropriate free block to give to the process)</li><li><span style="font-family: 'Helvetica Neue';">Because once the OS returns an address after a program’s call to malloc() it is generally very hard for anybody to know which pointers reference locations in that malloced memory. To implement relocation, we would need to know where all these pointers are and change their values to the relocated positions.</span></li><li><span style="font-family: 'Helvetica Neue';">Coalescing is the process of merging adjacent free blocks into a bigger free block.</span></li><li><span style="font-family: 'Helvetica Neue';">C will make sure to add a header file before the address returned to the user that will indicate the size of the malloc.</span></li><li><span style="font-family: 'Helvetica Neue';">Segregated lists are an alternative mechanism to free lists. If a particular application has one (or a few) popular-sized requests that it makes, keep a separate list just to manage objects of that size. All other requests are forwarded to a more general memory allocator.</span></li><li><span style="font-family: 'Helvetica Neue';">Buffer pools provide special reserved segments of memory that are only there to service allocations of respecified sizes (one pool per allocation size). All other allocations are handled by a general memory handler. Buffer pools trade memory amongst each other and the general memory area dynamically based on system needs.</span></li><li><span style="font-family: 'Helvetica Neue';">Paging is a mechanism that implements fixed sized partitions with the possibility of coalescing, relocation, compaction, and extension.</span></li><li>Paging offers more flexibility, and simpler free space management.</li><li>The page table offers address translation of virtual page frames to physical page frames. Page tables are a per-process data structure and is stored in RAM.</li><li>Series of steps:</li><ol><li>Divide the virtual address into its Virtual Page Number and offset.</li><li>Consult the Translation Lookaside Buffer</li><li>If Virtual Page Number is not present or is invalid go to Page Table</li><li>If VPN is in the TLB go look up the translated address in physical memory and return with result</li><li>If Page Table does not contain a valid instance of a translation (i.e. no valid bit) for the specified VPN it will generate a Segmentation Fault.</li><li>If the Page Table does contain the valid translation of the VPN then that Entry is loaded into the Page Table and we go back to step 1 (Note that then the TLB will experience another miss, but this time the Page Table will contain the adequate translation and then, finally, the translation will be loaded into the TLB, the memory access will be retried, and succeed).</li></ol><li>It contains:</li><ol><li>A valid bit — indicates wether the stored translation is valid. For example the page frames between the stack and heap might not yet be initialized, so the corresponding page table entries would be marked as invalid.</li><li>Protection bits — indicate read/write/execute privileges</li><li>Present bit — indicates wether the page is in RAM or on disk</li><li>Dirty bit — indicates wether the page has been modified since it was brought into memory</li><li>Reference bit — indicates if the page frame has been reference lately. Used for swapping</li></ol><li>This is stored in the Page-Table Base Register</li><li>In the Translation Lookaside Buffer</li><li>Temporal and Spacial Locality</li><li>In x86, the miss is handled entirely in hardware. However, newer systems usually use software entirely (specifically, through a trap handler).</li><li>CISC - Complex Instruction Set Computing; RISC - Reduced Instruction Set Computing</li><li>Upon a context switch, the full content of the TLB suddenly becomes invalid. Two approaches to fixing this:</li><ol><li>Upon every context switch flush the whole TLB</li><li>Use an Address Space Identifier (ASID) which specifies which process’s address space this translation is a part of. This way we do not have to take TLB entries in and out of the TLB upon context switches, which reduces context switch overhead.</li></ol><li>A TLB entry contains:</li><ol><li>valid bit — indicates wether the translation is still valid</li><li>protection bits — indicate read/write/execution privileges of the page frame</li><li>the actual physical address of the first byte in the page frame</li><li>Address Space Identifier — used to specify the which process the entry belongs to.</li></ol><li>Swapping. It swaps rarest used page frames out of RAM and swaps the currently needed page frames into RAM.</li><li>The PTE will contain a present bit which will indicate wether the specified page is in RAM. If it is not, it is first swapped in from RAM.</li><li>The page fault is activated upon a request to access a page frame out of RAM. Then, a page-fault handler takes care of loading the appropriate page into RAM. the PTE translation of that page frame is set to the new physical location of the page frame and the whole thing is retried.</li><li>The page-fault handler is implemented in the OS. Since going to disk is very slow, there is no added benefit to having the fetching be implemented in HW. Also, hardware would have to be aware of the I/O driver and how to use it, as well as the swapping mechanism. It does not know much about either.</li><li>The location of the page frame on disk will usually be stored in the respective Page Table Entry (even though it is not a RAM location)</li><li>10,000 — 100,000 times slower</li><li>A swap daemon is a thread that identifies when RAM gets too filled up with pages and then starts evicting pages until a certain amount of free RAM is available.</li><li>Low watermarks specify an amount of free RAM memory that is too low and compromising to the speed of the machine. At this point a swap daemon is activated and it evicts pages until the high watermark of free memory is reached.</li><li>There is a performance win in writing to disk in big batches (which happens when a lot of pages are evicted at the same time). Also, the machine does not get clogged by too many processes.</li><li>It is the time it takes for an average memory access in a process. It is mostly effected by the quality of the swapping policy.</li><li>The Least Recent Used policy.</li><li>A cold-start miss is a set of misses that necessarily occur whenever a set of pages for a new process need to be loaded. Initially, all these page frames will be misses and will go to disk.</li><li>The three types of cache misses are:</li><ol><li>Cold-start misses (compulsory misses) — due to previously unused data being cached</li><li>Capacity miss — occurs when a cache runs out of space</li><li>Conflict miss — when two different values map to the same cache location (does not occur with paging)</li></ol><li>Swapping policies:</li><ol><li>FIFO; Issue: what if the RAM supports 4 page frames, but the program keeps referencing page frames 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, … in that order.</li><li>Least Recently Used; good, but hard to implement</li><li>Least Frequently Used; frequently used pages will be used in the future a lot as well; Issue: 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4 (we don’t need 1, 2, 3 anymore even though they were frequent).</li></ol><li>Each process will have attached to it a number which represents how many page frames it can have loaded into memory. The more memory a program needs, the bigger its workload gets.</li><li>Clock algorithm: Have a circular list of all pages and have a current pointer to one page. Look at reference bit to see if it is set, if it is unset it and go to next page frame in list and repeat. If it is not set, swap it out and bring the new page in its place.</li><li>It prefers to evict clean (vs dirty) page frames. This is done because clean pages have the same content on disk, so no writing is necessary and eviction is free. This can be done by just skipping all the pages that have the dirty bit set.</li><li>Demand paging is the simplest approach (bring page frame into RAM if it is requested by a process only). Also, prefetching is possible (here we try and predict which pages the running processes are going to need).</li><li>Clustering means writing multiple pages out to disk at the same time. This batch writing improves write-to-disk speed.</li><li>Thrashing is a state of a machine in which pages are constantly being swapped in and out of RAM. It is caused by page frame demand that is bigger than the available RAM.</li><li>Admission control is a policy that specifies that it is better to not thrash by having some processes not run at all, then thrash and try to run all the processes at once. Admission control might kill critical processes.</li><li>The thread control block (vs the process control block):</li><ol><li>Program counter</li><li>Registers</li><li>Stack</li></ol><li>Multiple threads can execute within one process therefore sharing one address space. Also, when you only have a process there is only one stack, but with multiple threads in one process, each thread will have its own stack within the same (the process’s) address space.</li><li>Parallelism and ease and speed of sharing data (specifically, you can have one thread wait for I/O, while others are doing something else; this means the application does not block during I/O waits)</li><li>Processes are better when running logically separate tasks with little or no data sharing.</li><li>A rece condition is a circumstance in which the result of a computation depends on the order of execution of all the threads involved.</li><li>The critical section</li><li>Mutual exclusion and notification of asynchronous completion. The following are possible solutions to race conditions:</li><ol><li>Interrupt disable</li><li>Locking</li><ol><li>Spin Locking</li><li>Condition variables</li></ol><li>Use only atomic instructions</li><li>Avoiding the use of shared data</li></ol><li>Deadlocks</li><li>Coarse-grained locks allow only one thread to access a locked area of code. Fine-grained locks allow for a specific number of threads to access the critical section simultaneously.</li><li>The following might be useful quality parameters for locks:</li><ol><li>Correctness: Does it provide the required mutual exclusion?</li><li>Fairness: Does each thread waiting on a lock get to access it?</li><li>Performance: What is the overhead of the locking mechanism?</li></ol><li>The following are disadvantages of interrupt disables:</li><ol><li>They work only on single-processor systems</li><li>Requires privileged instruction access</li><li>Bugs can cause disastrous problems with the scheduler</li><li>Scheduler issues: lack of preemption, loss of interrupts, I/O buffer flushing</li><li>Slow</li></ol><li>A few:</li><ol><li>The compiler might optimize away the set/reset of the flag since it is not used in any other context</li><li>It is incorrect: It is impossible to do without non-atomic instructions, which means the locking mechanism becomes subject to race conditions itself.</li><li>The implementation is slow since it will only allow for spin locks</li></ol><li>Spin locks exhibit bad performance because cycles are spent on a thread that only checks for a lock without doing any real work. The scheduler must be preemptive because non-preemptive schedulers would never exit from a spinning thread. Fairness is poor: there is no guarantee that a waiting thread will ever grab a lock and the lock access pattern is random and dependent on the scheduler.</li><li>Like waiting at the DMV: You want access to a locked segment, you get a number, when the lock gets to your number you can enter. This is an upgrade compared to spin locks, because it provides fairness.</li><li>When the thread yields, it allows other processes/threads to be scheduled, unfortunately there is still an overhead of 2 context switches. Also, fairness is not at all better.</li><li>Use park() and unpark(threadId). park() sends the thread to sleep (so it is not scheduled). The thread wakes up when unpark() is called with its ID. This requires us to use queues alongside mutex variables. Thus, when we unlock() we will need to update the lock and then update the queue as well (which we can implement with traditional spin locks). Same issue when a thread wants to get in line (in the queue) waiting to get scheduled, <span style="font-style: italic;">i.e.</span> a spin lock must be run to ensure only one thread is added to the queue at a time. However, there is significantly less spinning here (only twice per thread — once on lock, once on unlock).</li><li>Assume T1 has higher priority than T2, and that the scheduler interprets this as T1 must run whenever it is not blocked. Assume T1 is blocked, and that since T2 is running, T2 enters a lock. Then T1 gets unblocked, gets to the locked code and starts spinning indefinitely, never letting T2 finish. One solution is priority inheritance: If a higher priority thread is waiting on a lower priority one, increase the priority of the lower priority one to the higher priority one.</li><li>The wakeup/waiting race occurs when one thread waiting on a locked resources goes to sleep assuming it will be woken up when the resource is available. If just before it actually goes to sleep (since going to sleep is not atomic this is possible) the lock it is waiting for gets unlocked and therefore sends a signal to all waiting threads, the about-to-sleep thread will not be one of them. If there are no other threads waiting, the about-to-sleep thread might never get to run.</li><li>A condition variables is a queue threads can put themselves onto. Once the condition of a condition variable is realized, some or all of the threads might be informed. There is no spinning, threads go into waiting state and are woken up when condition is met.</li><li>Possible issues: The parent can go to pthread_join() before child finishes as well as the child can finish before parent reaches pthread_join(). Both cases must be covered. This can be done by having one atomically incremental variable that just signals if the child is done. If the child finishes before parent reaches pthread_join(), it will just read the value of done and go on. For the case that the parent reaches pthread_join() before the child finishes, we implement a pthread_cond_wait inside the pthread_join() so that the parent will wait on the conditional variable which will be set by the child once it exits. Note that both functions need to be surrounded by a lock to the same mutex. Not having either the regular done variable or the conditional variable breaks one of the possible execution flows.</li><li>Always lock when signaling with conditional variables.</li><li>Because:</li><ol><li>The cost of building different versions of applications for different hardware platforms is very high (which is a problem for software developers)</li><li>Hardware sales are driven by applications they can support (which is why hardware developers like compatible software)</li></ol><li>For proper compatibility we need:</li><ol><li>Backwards compatibility must be employed</li><li>Software upgrade do not break the existing applications</li></ol><li>The benefits of standardization are:</li><ol><li>Specifications are usually an agreement between many different experts. Specifications are therefore probably rather well thought-through.</li><li>Because the contributors work for many different organizations, with different requirements, so the standard will probably be neutral with respect to different platforms.</li><li>They tend to have clear and complete specifications and well developed conformance testing procedures.</li><li>They give technology suppliers considerable freedom of implementation as long as the specified external behavior is maintained.</li></ol><li>The disadvantages of standardization are:</li><ol><li>Many possible implementations are not possible due to standardization restrictions.</li><li>Applications that use non-standard features it might very easily break in a future implementation of the standard.</li><li>It is difficult to change standards to meet new requirements. Any change, no matter how beneficial will affect a segment of the users.</li></ol><li>The drawbacks of keeping an interface proprietary are:</li><ol><li>If a competing open standard evolves, and ours is not clearly superior, it will eventually lose and our market position will suffer</li><li>Competing standards will fragment the market and reduce adoption</li><li>With no partners, we will have to shoulder the full costs of development and evangelization.<br/>While the benefits are</li><ol><li>freedom to change the interface if needed</li><li>having a possible competitive advantage due to becoming a first only provider</li><li>Not being forced to re-engineer existing implementations to bring them into compliance with a revised, committee-adopted interfaces.</li></ol></ol><li>API is a specification that provides developers with an interface for using lower-level services.</li><li>The benefits of APIs are:</li><ol><li>To software developers: An application written to a specific API can easily be recompiled and will correctly execute on any platform that supports that API.</li><li>To hardware developers and OS developers: The benefit for platform suppliers is that any application written to the supported APIs will be easily portable to their platform.</li></ol><li>The Application Binary Interface (ABI) are necessary to bind a piece of software to a specific platform (specifically, a Application Programming Interface (API)). It provides the specification and services that will allow the program to run on a specific machine. A program compiled for a specific ABI will run on any platform that supports that ABI without any need for recompilation. </li><li>Editable text in some programming language</li><li>A relocatable object module is a set of compiled or assembled instructions created from individual source modules</li><li>Libraries are collections of object modules from which we can fetch instructions for programs that use them.</li><li>Load modules are complete programs ready to be loaded into memory by the OS and executed by the CPU.</li><li>It might be useful to implement the following in assembly:</li><ol><li>performance critical string and data structure manipulations (user mode)</li><li>routines to implement calls into the OS (user mode)</li><li>CPU initialization (kernel mode)</li><li>first level trap/interrupt handlers (kernel mode)</li><li>synchronization operations (kernel mode)</li></ol><li>The linkage editor reads a specified set of object modules and places them into a virtual address space (noting where each was placed). It also takes note of all unresolved references it encounters. It then searches a specified set of libraries to find object modules that can satisfy those references and places them in the evolving virtual address space as well. After placing all the object modules into the virtual address space, it finds every reference to a relocatable or external symbol and updates it to reflect its address in virtual memory. After the linkage editor is done, the program is ready for loading and subsequent execution.</li><li>A program loader is usually part of the OS. It checks the load module and loads it into physical RAM.</li><li>The Executable and Relocatable File (ELF). Each ELF contains:</li><ol><li>A header section describing the types, sizes, and locations of all other sections.</li><li>Code and Data sections, each containing bytes of code or data that are to be loaded (contiguously) into memory.</li><li>A symbol table which lists all external symbols defined or needed by this module.</li><li>A collection of relocation entries, each of which describes:</li><ol><li>the location of a field that requires relocation</li><li>the width/type of the field to be relocated</li><li>the symbol table entry, whose address should be used to perform the relocation</li></ol></ol><li>The following needs to happen to create a runnable program from an ELF:</li><ol><li>Resolution of all unresolved external references</li><li>Loading — laying the text and data segments from all relevant object modules into a single address space</li><li>Relocation — go through all the relocation entries in all object modules and bind them to adequate addresses based on the choice of virtual address space layout.</li></ol><li>Symbol tables are used in executables for debugging purposes. If a bug occurs at a specific address, referring to the symbol table will help the OS report a meaningful error that tells us something like: The error occurred this many lines into this function (because it know where the function is in memory due to the symbol table).</li><li>The disadvantages of static linking are:</li><ol><li>all the used library code must be loaded into each of the executables. If the library is used in multiple programs each one’s executable will contain the same copy of the library code.</li><li>library updates are not automatically used in your code. You must recompile</li></ol><li>Shared libraries are implemented using the following mechanism:</li><ol><li>Reserve an address for each shared library in physical addresses (easy in 64-bit architectures).</li><li>Linkage edit each shared library into a read-only code segment, loaded at the address reserved for that library</li><li>Assign a number to each routine, and put a redirection table at the beginning of that shared segment, containing the addresses of each routine in the shared library.</li><li>Create a stub library, that defines symbols for every entry point in the shared library, but implements each as a branch through the appropriate entry in the redirection table (the stub library will also contain a symbol table that informs the OS what shared library segment this program requires).</li><li>Linkage edit the client program with the stub library</li><li>When the OS loads the program into memory, it will notice that the program requires shared libraries, and will open the associated code segments and map them into the new program’s address space at the appropriate location.</li></ol><li>The advantages of shared libraries include:</li><ol><li>A single copy in memory of a shared library can be used by multiple programs.</li><li>The version of the shared library that gets loaded is decided not at compile time but at load time, so library updates can be used without recompilation.</li><li>Because all calls between the client program and the shared library are vectored through a table, client programs still work even if the implementation is changed or if new modules are added to a library.</li><li>With correct coding of the stub modules, it is possible for one shared library to make calls into another</li></ol><li>The disadvantages of shared libraries are:</li><ol><li>The shared segments contain only read-only code, so routines cannot have any static data. Short lived data can be allocated on the stack, but persistent data must be maintained by the client.</li><li>The shared segment will not be linkage edited against the client program, and so cannot make any static calls or reference global variables to/in the client program.</li><li>There may be very large/expensive libraries that are seldom used; Loading/mapping such libraries into the process’ address space at program load time unnecessarily slows down program start-up and increases the memory footprint. In some cases, it might be preferable to delay the loading of a module until it is actually needed.</li><li>While loading is delayed until program load time, he name of the library to be loaded must be known at linkage editing time. </li></ol><li>Dynamically loaded libraries:</li><ol><li>The application chooses a library to be loaded and asks the OS to load the library into its address space.</li><li>The OS provides entry points for initializing and shutting down the library. Then the software runs this code and the DLL is bound.</li><li>The software can now use the dynamically established vector of service entry points to access modules of the DLL.</li><li>When the program is done with the DLL, it can call the shut-down method to let the OS unload the library.</li></ol><li>DLLs are used by the OS to dynamically load some device drivers of file systems.</li><li>A compromise between a shared library and DLL is implemented using Program Linkage Table (PLT) which is loaded into the client load module:</li><ol><li>The PLT entries are initialized to point to calls to a Run-Time Loader</li><li>The first time one of the entry points is called, the stub calls the Run-Time Loader to open and load the appropriate DLL.</li><li>After the library was loaded, the PLT entry is changed to point to the appropriate routine in the newly loaded library.</li></ol><li>The benefits of DLLs are:</li><ol><li>We can choose which library to load based on conditions in our code</li><li>DLLs only consume memory of the process as long as they are used (they can be unloaded)</li></ol><li>Shared libraries are loaded faster than DLLs.</li><li>The OS must:</li><ol><li>allow parameter passing on the stack</li><li>saving return address on the stack and transferring control to subroutine</li><li>register saving (in order to be restored later)</li><li>allocating space for local variables of subroutine</li></ol><li>The OS must:</li><ol><li>put the return value on the stack</li><li>delete the local variables of the subroutine</li><li>restore registers of the caller</li><li>get and set the program counter </li></ol><li>The caller has to pass the parameters. It must also put the return address on the stack. The subroutine save the caller’s registers. The subroutine has to remove the parameters and its local variables off the stack. It must put the return value onto the stack.</li><li>The differences between interrupts and subroutine calls are:</li><ol><li>A subroutine call is initiated by the software and something is expected by the caller. The interrupt is usually handled without the software noticing anything.</li><li>The caller does not anticipate an interrupt.</li><li>The linkage conventions of subroutine calls are managed by the software. Interrupt linkage are handled by hardware.</li></ol><li>There is a number associated with each external interrupt or execution exception. There is a table, initialized by the OS, that associates a program counter and processor status word with each possible external interrupt or execution exception. When an interrupt happens:</li><ol><li>The CPU uses the associated interrupt/trap number to index into the interrupt/trap vector table</li><li>The CPU sets the new program counter (and saves the state of the interrupted process onto the stack)</li><ol><li>the assembly code called the first level interrupt handler is executed:</li><ol><li>the first level handler chooses the second level handler and makes a regular procedure call to it</li><li>then the second level handler returns</li><li>the first level handler executes a privileged return from interrupt instruction</li><li>the CPU re-loads all of the interrupted process’s data and normal execution continues</li></ol></ol></ol><li>Interrupts require a loading of a new processor status word, which often implies the processor will go into kernel mode. This will require a new address space to be loaded. This invalidates all the caches, and produces many cold hits. This makes them 100x to 1000x more expensive than a regular subroutine call.</li><li>OS layering is the method by which the operating system offers services. Namely, the idea that higher level services that are very useful to application developers are built out of simpler block that are in turn layered on top of even simpler layers and that all these layers eventually map to very simple hardware.</li><li>There are a couple of services offered to application software:</li><ol><li>Libraries</li><li>Subroutine calls</li><li>System calls (asking for privileged instruction from the kernel)</li><li>Under the covers (not directly visible — fans, scheduling, USB ports, software updates…)</li><li>Messages (exchanged messages with server)</li><li>Middleware (middleware is a part of the application or service platform, but not part of OS)</li></ol><li>The user mode instruction set is defined and implemented by hardware (i.e. it is ISA specific). The ABI is defined and implemented by software (i.e. it is OS specific). For example, the ABI deals with things like alignment, datatype size, object file formats, calling conventions, how system calls are made (specifically system call numbers). ISA is more concerned with what a specific bitstream means to the hardware. ISA is locked at hardware design time, while the ABI is locked at OS design time.</li><li>Compliance (of the application developers to the standard specification) and stability (of the implementations of the standard to comply with the specifications).</li><li>Drawbacks of layering are:</li><ol><li>Performance overheads due to all the subroutine calls and managing the linking</li><li>Lower layers may impose limits on what a higher layer may do</li></ol><li>Soft and Hard scheduling.</li><li>Real time scheduling performance is measured by:</li><ol><li>timeliness - how closely are timing requirements met</li><li>predictability - how much deviation is there in delivered timeliness</li></ol><li>Drawbacks of semaphores include:</li><ol><li>Easy to deadlock</li><li>Must block to check lock</li><li>No reader/writer shared access support</li><li>No way to deal with wedged V operations</li><li>No way to deal with priority inheritance</li></ol><li>Mutexes are used to lock code segments (not objects).</li><li>Enforced locking is done within the implementation of object methods and are guaranteed to happen. Advisory locking allows the user to lock, but the user is  not obligated to do so.</li><li>Mutexes and flocks()</li><li>lockf() (if the underlying file systems implements an enforced lock!)</li><li>lockf() may be implemented to be enforced. lockf() locks the file itself, while flock() locks per file descriptor.</li><li>Convoy formation and priority inversion.</li><li>Locking is usually implemented as a system call which implies all the overheads of system calls. So, since critical sections are usually kept to be small, the overhead of a lock system call is usually greater than the cost of executing the critical section.</li><li>The convoy is permanently present if the time spent waiting on a lock becomes as big as the mean inter-arrival time</li><li>Priority inheritance.</li><li>Contention can be reduced by:</li><ol><li>Eliminating critical sections by eliminating shared resources and using atomic instructions.</li><li>Eliminating preemption during critical section (may require disabling interrupts)</li><li>Reduce time spent in critical section (do all system calls possible outside the critical section, minimize code in critical section)</li><li>Reduce frequency of entering the critical section (as in sloppy counters)</li><li>Remove requirement for full exclusivity</li><li>Reduce exclusive use of the serialized resource (distinguish between read/write locks)</li><li>Spread requests over more resources</li><ol><li>Coarse grained (one lock for many objects) vs fine grained locking (one lock per object)</li></ol></ol><li>Sloppy counters solve contention by reducing the frequency of entering the critical section.</li><li>Read/write locks</li><li>Having multiple linked lists, each with its own lock instead of having one big linked list with one lock.</li><li>We make the following distinction between resource type in context of deadlocks:</li><ol><li>Commodity resources: Clients need an amount of it and deadlocks result from overcommitment. Avoidance can be done in resource manager.</li><li>General resources: Clients need a specific instance of something and deadlocks result from specific dependency relationships. Prevention usually done at design time.</li></ol><li>The following conditions must be met for there to be a chance of deadlock:</li><ol><li>Mutual exclusion</li><li>Incremental allocation</li><li>No preemption</li><li>Cyclical dependency</li></ol><li>By using reservations: Resource manager tracks outstanding reservations and grants reservations only if resources are available. Over-subscriptions are detected before a process gets a resource. Clients must be prepared to deal with failures, but no deadlocks. Dilemma: over-booking vs. under-utilization</li><li>Incremental allocation can be eliminated by requiring all processes to release all held locks before they block and wait to acquire a new lock.</li><li>Preemption can be used for resource confiscation: Instead of giving resources for indefinite time, lease them out with a timeout stamp. Then the OS seizes the resources back once the lease expires.</li><li>Seizing resources usually involves changing the process’s address space. This means it is probably safest to kill the resource. Killing resources is not good.</li><li>Circular dependencies can be addressed by implementing a total ordering of all resources and then imposing a discipline whereby you can only lock a resource that is bigger than all other locked resources. This might require a lock dance.</li><li>Deadlock detection and recovery. Must maintain a wait-for graph of all resources that can be locked. Every time a lock is requested this graph must be updated.</li><li>Health monitoring involves pinging a process every so often and seeing if it has made any progress. Is process doesn’t progress, declare that it is hung. This is easy to implement and detects a wide range of problems.</li><li>The problems health monitoring addresses include:</li><ol><li>Live-lock: Process is running but won’t free a resource until it gets a message. The message is never sent because whoever needs to send it is not going to until it gets the resource the first process is holding.</li><li>Sleeping beauty waiting for prince charming: a process is blocked waiting for a completion that will never happen.</li><li>Priority inversion hangs</li><li>Note that these are not strictly speaking deadlocks. Specifically they cannot be detected with a wait-lock graph.</li></ol><li>The following are the possible implementations of health monitoring:</li><ol><li>Look for obvious failures (exits, core dumps)</li><li>Passive observation (is the process consuming CPU time, is it doing network or disk I/O)</li><li>External health monitoring (Ping the process, null requests)</li><li>Internal instrumentation (white box audits, exercisers)</li></ol><li>Hung process usually has to be killed. However, usually a whole group of processes has to be killed if one process is hung. So, prior to execution, define inter-group dependencies and groups so that you know what to kill.</li><li>Failure recovery methodologies include:</li><ol><li>Retry failed portion</li><li>Roll-back failed operation and return error</li><li>Continue with reduced capacity/funcitonality</li><li>Automatic cold/warm/partial restart</li><li>Implement escalation mechanism for failed recoveries (if simple recovery doesn’t work, restart/reboot more and more process groups/machines)</li></ol><li>Monitors are locks implemented on a class level, i.e. the class has a semaphore associated with it; each time an object or method of the monitored class is called, the whole class is locked. This gives high confidence of adequate protection and makes code simple (synchronization implemented by the compiler, we just need to tag the class as a monitor)</li><li>Synchronized keyword can be used to tag serialized methods inside classes. When a class has at least one synchronized method, it will associate a lock with each object of that class. If a synchronized method is called on that object, the lock is enforced and no other method or variable associated with the object can be accessed. Once the synchronized method returns it releases the lock. Any nested calls inside synchronized methods to other methods of the same class are allowed and no new locks are created for this purpose.</li><li>The division of functionalities should be as follows:</li><ol><li>OS code should contain common functionality</li><ol><li>Caching</li><li>File systems code not tied to a specific device</li><li>Network protocols above physical/link layers</li></ol><li>Drivers should contain functionality specific to their device</li><ol><li>Implementation details that differ between different hardware pieces</li><li>Things that uniquely pertain to a certain piece of hardware</li></ol></ol><li>Through the bus and interrupts. Devices usually do their own thing independently and in parallel with the CPU. The only communication method between them is interrupts.</li><li>They are the same except that traps come from inside the CPU, while interrupts are caused externally to the CPU. Interrupts— in contrast to traps — can be disabled. This means that devices can be told when they are allowed to generate interrupts. The CPU can also stack interrupts into a pending queue and not respond to them immediately.</li><li>Lately, I/O traffic has shifted focus from disk I/O to network and video I/O</li><li>In contrast to the historic goals of maximizing throughput and IOPS, now we focus on lowering latency, and increasing scalability, reliability, and availability.</li><li>Keeping them busy, i.e. reducing idle time.</li><li>The following:</li><ol><li>Direct Memory Access (DMA), which allows for a device to communicate with RAM over the bus without consulting the CPU. This allows data to flow between RAM and devices at bus/device/memory speed. Note that the bus can only be used for one thing, so if it is used for DMA it cannot be used by the CPU to access RAM. However, if the CPU has to access RAM, our computer will run slow anyways and the increase in performance due to high device throughput is less relevant.</li><li>Queueing tasks for devices: Each device has a buffer/queue of tasks it needs to perform so that it can always work.</li></ol><li>Task completion on a device triggers the following events:</li><ol><li>Device controller generates completion interrupts</li><li>OS accepts interrupt and calls handler</li><li>Interrupt handler posts completion to requester</li><li>Interrupt handler selects and initiates next transfer</li></ol><li>Each device has a per-operation overhead</li><ol><li>Instructions to set up operation</li><li>Device time to start new operation</li><li>Time to services completion interrupt</li><li>high seek/rotation overheads with disks</li></ol><li>Most I/O requests require data to travel to/from RAM. That data needs to be held in memory. Buffers are used to store data from RAM that is ready to be sent to devices as well as to store data from devices that is ready to be stored in RAM.</li><li>Couple of options:</li><ol><li>Maintain a cache of recently used disk blocks</li><li>Accumulate small writes and flush them out only when block fills</li><li>Read full blocks and deliver only requested portions</li><li>Use read-ahead, i.e. read/cache blocks that are not yet requested.</li></ol><li>Deep request queues are good for disk because</li><ol><li>We maintain high disk utilization (low idle time)</li><li>We reduce mean seek/rotation time</li><li>We may be able to combine adjacent requests</li><li>We can avoid writing completely sometimes</li></ol><li>Deep request queues can be achieved by:</li><ol><li>Many processes making requests</li><li>Individual processes making serial requests</li><li>Use read-ahead for expected data requests</li><li>Use write-back cache flushing</li></ol><li>Double-Buffered Output provides multiple buffers and allows writes to disk to be queued up. Allows disk to start writing to itself right after it finishes the last write since there is another buffer waiting. This also allows the application to queue writes without waiting for disk to finish writing to itself (and using the buffer) since there is another buffer that the disk is not using. If we are CPU-bound, we don’t have to block to populate the write buffer. If we are I/O bound, we improve device throughput because writes can immediately follow each other (however might have to block process anyways).</li><li>Double-buffer input are not useful if we only have one process reading from the device because that process will most probably block until the read is complete (unless we use read-ahead!). However, we might use double-buffer input when we have multiple processes requesting reads from the same device at the same time. Also, if we are using read-ahead we can even use multiple buffers for a single read request.</li><li>Scatter/gather describes the issue that devices read/write from/to buffers that contain contiguous blocks of memory, however, the physical memory is stored in many different and non-contiguous pages due to paging. Scatter is the process of reading from device to multiple pages. Gather is the process of writing from multiple pages into a single devices.</li><li>The scatter/gather mechanism could be implemented by:</li><ol><li>Using physical buffers: Copy all required pages into physical buffers that will store data contiguously.</li><li>Split actual requests into chain-scheduled page requests.</li><li>Handle scatter/gather in the I/O MMU</li></ol><li>DMA is designed for large contiguous transfers, while many devices naturally have many small sparse transfers.</li><li>Memory mapped I/O implies that registers and caches of devices are mapped to virtual memory addresses in the process, and that the process can read/write to these locations with the same instructions as with RAM.</li><li>DMA vs. Memory Mapped I/O:</li><ol><li>DMA advantage:</li><ol><li>Does not block and waist CPU cycles; further, device does not have to wait for CPU to perform higher priority tasks before it reads/writes to RAM. This implies better utilization of both the devices and the CPU.</li></ol><li>Memory-mapped I/O advantage:</li><ol><li>No per-operation overhead, but every transfer goes through the CPU.</li></ol></ol><li>DMA is better if there is a small number of relatively large read/write requests from device to RAM. Memory mapped I/O is better is there is a greater number of relatively small read/write requests from device to RAM.</li><li>The Device Driver Interface is an interface provided by the operating system to third party developers that requires certain types of operations to be implemented by that device type. E.g. all disks must have read/write/open… methods implemented. DDIs could be specific to a certain type of device (e.g. serial devices require a receive character method, while network DDIs require an implementation of a transmit function) or they could be common to all devices (e.g. all device drivers must have initialization methods implemented).</li><li>The abstractions provided by a driver include:</li><ol><li>Encapsulation of specifics of rudimentary DDI operations</li><li>Encapsulation of optimization</li><li>Encapsulation of fault handling</li></ol><li>The Driver/Kernel interface is an interface of services provided to drivers by the kernel. These include:</li><ol><li>Memory allocation, data transfer and buffering</li><li>I/O interrupt and resource management</li><li>DMA services</li><li>Synchronization, error reporting</li></ol><li>All linux device drivers fall within the following super-classes:</li><ol><li>Block devices: Devices that deal with a block of data at a time (e.g. disk drives, random access devices).</li><li>Character devices: Devices that read/write one byte at a time; either stream or record structured; either sequential or random access; (e.g. keyboards, monitors, most other devices)</li><li>Network devices: Devices that send/receive data in packets (Ethernet cards, Bluetooth devices)</li></ol><li>Block devices require more elaborate services including:</li><ol><li>Buffer allocation</li><li>LRU management of buffer cache</li><li>Data copying services for the buffers</li><li>Scheduled I/O</li><li>Asynchronous completion</li><li>Have to support paging, swapping, and file systems</li><li>Require higher performance (typing at keyboard is slow so one byte at a time is OK, this is not true for disk)</li></ol><li>The major device number specifies which device driver to use; the minor device number specifies uniquely a device among other devices that use the same driver.</li><li>Through the file system. A special tag associated with a file implies that that file is actually associated with a specific device and its memory. All system calls (e.g. open/write/read) will actually go through the device driver implementation instead of the standard OS system call implementation.</li><li>When a static synchronized method is accessed in Java, we lock the intrinsic lock of the class itself. So, access for a class’s static fields is controlled by a lock intrinsic to the class, not the object.</li><li>A monitor is a thread-safe class, object, or module that uses wrapped mutual exclusion to allow safe access to methods and variables by multiple threads. The defining feature of a monitor is that at any point in time at most one thread can be executing a specific method associated with it.</li><li>Condition variables are used to prevent deadlocks within monitored methods. For example, if a thread executing a particular monitored method, but must wait of a condition before continuing execution it can perform an atomic wait m, c operation. This operation will release the mutex m, block the thread, and only continue once the condition c is met and mutex m is available. If these the mutex is available and condition c met, our thread will lock the mutex and continue execution.</li><li>Dispersion indices include:</li><ol><li>Range</li><li>Standard Deviation</li><li>Confidence intervals</li></ol><li>The OS:</li><ol><li>Controls access to application memory</li><li>Controls process scheduling</li><li>Controls resources</li></ol><li>Security is a policy. Protection is a mechanism that implements the policy</li><li><br/></li><li><br/></li><li><br/></li><li><br/></li><li>AES - Advanced Encryption Standard</li><li>Goals of Distributed systems include:</li><ol><li>Scalability and performance</li><li>Improved reliability and availability</li><li>Ease of use, with reduced operating expenses</li><li>Enabling collaboration</li></ol><li>Deutsch’s seven fallacies of network computing:</li><ol><li>The network is reliable</li><li>Latency is zero</li><li>Network bandwidth is infinite</li><li>Network is secure</li><li>Topology of network does not change</li><li>There is one administrator for the whole network</li><li>Cost of transporting additional data is zero</li></ol><li>Loosely Coupled Systems imply:</li><ol><li>Parallel group of independent computers serving similar but independent requests</li><li>Minimal coordination and cooperation required</li></ol><li>Loosely Coupled Systems are a good solution due to:</li><ol><li>Scalability and prices performance</li><li>Availability guarantees</li><li>Ease of management and reconfiguration</li><li>Horizontally scalable</li></ol><li>Horizontal scalability implies:</li><ol><li>Node are largely independent of each other</li><li>Easy to add new nodes</li><li>High reliability (crashing nodes only reduce capacity, but do not kill entire system)</li></ol><li>Elements of Loosely Coupled Systems:</li><ol><li>Servers are independent (may share common backend database)</li><li>Front-end switch distributes incoming requests to servers. Responsible or load balancing and fail-over.</li><li>Service protocols are stateless with idempotent operations.</li></ol><li>Cloud computing implies a single loosely coupled distributed system for everything. Nodes connected to the internet and to each other over high-speed LAN. Use nodes to run everything from small requests to huge batch computations.</li><li> </li><li> </li><li> </li><li> Stubs on the client side translate system calls into packeted messages. On the server side, the packeted messages are translated into actual function calls by the server stub.</li><li>Distributed consensus is hard due to:</li><ol><li>Spatial separation: No shared memory for locking mechanism; different OSes control different nodes.</li><li>Temporal separation: Cannot impose total ordering</li><li>Independent failure modes: One node may die, while others continue to work.</li></ol><li>Leases are provided upon request by the resource manages; they are only valid for a specified amount of time; handles many modes of failure not handled by locks.</li><li>Leases can be tricky:</li><ol><li>We need to make sure that when a lease is revoked, the resource it locks is left in a reasonable state (all-or-nothing atomicity)</li></ol><li>Security is harder in distributed systems because:</li><ol><li>The node OS cannot alone provide security guarantees since a distributed model requires inter-node communication outside the OS.</li><li>Authentication is harder: The node itself may not have proof of authenticity and has to trust other nodes.</li><li>The wire connecting the server to client is insecure</li></ol><li>Goals of network security include:</li><ol><li>Secure conversation: Privacy (nobody is listening in) + Integrity (you get what the other guy sends you)</li><li>Authentication: Reliable identification of replays or forgeries.</li><li>Non-repudiation</li><li>Availability: Service is not denied to users that should receive it</li></ol><li>Network security is implemented through:</li><ol><li>Public key cryptography (primarily for authentication)</li><li>Symmetric cryptography (for protecting bulk transport)</li><li>Cryptographic hashes (used to ensure integrity)</li><li>Digital signatures and certificates </li><li>Firewalls (filtering technology)</li></ol><li>The less we use a key the less encrypted data points that an attacker will potentially have access to.</li><li>Goals of remote file access include:</li><ol><li>Transparency: Indistinguishable from local files</li><li>Performance: </li><ol><li>for client - as fast as local disk access</li><li>scalability - unaffected by number of clients</li></ol><li>Cost:</li><ol><li>for client - less than local disk storage</li><li>operational - zero (no administration required)</li></ol><li>Capacity:</li><ol><li>Unlimited</li><li>Availability: 100% (no failures or server down time)</li></ol></ol><li><br/></li><li>Remote file access architectures include:</li><ol><li>Client/server</li><ol><li>Thin client (all work done centrally on server)</li></ol><li>Remote file transfer</li><ol><li>Explicit commands to remotely copy files (scp, rsync)</li><li>Implicit remote data transfer</li><ol><li>Browser (transfer through HTTP)</li><li>Email clients (move files with IMAP/POP/SMT)</li></ol><li>Requires no OS support, efficient. BUT high latency, lack of transparency.</li></ol><li>Remode disk access</li><ol><li>SCSI over ethernet: Client driver turns read/write requests into network requests; servers deamon processes requests; moderate performance, high scalability, low cost</li><li>THE model for virtual machines</li></ol><li>Remote file access</li><ol><li>Exploits plug-in file system architecture</li><li>Client-side file system is local proxy</li><li>Translates file operations into network requests</li><li>Good app level transparency; good encapsulation; multi-client file sharing support; potentially good performance and robustness</li><li>BUT, part of implementation must be in OS; both client and server side fairly complex</li><li>THE model for client/server storage</li></ol><li>Cloud</li><ol><li>Clients access services rather than resources</li><li>Clients do not see individual servers</li></ol></ol><li>Remote implies client talks to primary server. Secondary server may take over if primary fails. Advantage is simplicity. Distributed file systems spread data across many nodes. Client may talk directly to many of them. Advantage is performance and scalability. Disadvantage is a lot of added complexity.</li><li>Having each node independently authenticate and authorize a user is not scalable. BUT, it is simple.</li><li>Other options of authentication:</li><ol><li>Authenticate through mutually trusted third-party server (Kerberos).</li></ol><li>Two approaches:</li><ol><li>Authentication service returns credentials; these credentials checked on server; advantage: authentication services does not posses the ACL; could be used for subsequent authorization</li><li>Authentication service returns capability; which server verifies by signature; advantage: server does not know about which client is issuing request, access can be granted all-at-once.</li></ol><li>Reliability is  a high degree of assurance that the server will behave correctly. Availability is a high degree of assurance that the service will be available.</li><li>Reliability can be achieved through:</li><ol><li>Redundancy (RAID mirroring/parity/erasure coding); copies on multiple servers</li><li>Automatic failure recovery (remote data copies become available again; redundancy losses are made up on the fly)</li></ol><li>Two recovery mechanisms:</li><ol><li>Client driven recovery:</li><ol><li>Client detects server failure due to connection error</li><li>Client reconnects or finds server that is up</li><li>Client reestablishes session</li></ol><li>Transparent failure recovery:</li><ol><li>System detects server failure (health monitoring)</li><li>Successor assumes primary’s IP address so that it masks the failed node</li><li>State reestablishment; Two options:</li><ol><li>Successor recovers primary’s last check-point</li><li>Stateless protocol</li></ol></ol></ol><li>Stateless: HTTP; Stateful: TCP</li><li>Stafeful protocols require that recovery servers obtain full state of failed server prior to processing any requests (this might be hard and slow).</li><li>Idempotency. If Akn message does not get received by client, client does not have to worry if the server processed its request or didn’t; it can reissue to command until it receives akn.</li><li>Network imposes following constraints to performance:</li><ol><li>Throughput implications</li><li>Delay implications (acknowledgments may not be sent in time due to network delay)</li><li>Packet loss implication (loss rate high -&gt; need acknowledgment -&gt; increased throughput for same number of operations)</li></ol><li>Read performance improved through both client-side and server-side caching.</li><li>AFS uses whole file caching. This is due to high network latency.</li><li>Mirroring in distributed systems could be implemented through:</li><ol><li>Multi-host vs. multi-disk mirroring: Protects against both host and disk failure, but adds additional network traffic</li><li>Mirroring by primary: Primary becomes bottleneck; better to move mirroring to back-side network</li><li>Mirroring by client + parity/erasure code computation on client CPU</li></ol><li>Direct Data Path implies that the primary server tells client where the requested data resides; then the client communicates directly with responsible node. Implications:</li><ol><li>Throughput: load balanced over many nodes</li><li>Latency: No added latency due to primary bottleneck</li><li>Scalability: Fewer messages go over main network; much less data through primary server.</li></ol><li>Availability is Mean Time to Failure / (Mean Time to Failure + Mean Time to Recovery).</li><li>Mean Time to Recovery can be improved by:</li><ol><li>Detect failures quicker</li><li>Transfer to secondary quicker</li><li>Recover recent operations quickly</li><li>Rebind client quickly</li><li>Re-establish session state quickly</li></ol><li>Messages per client per second can be minimized</li><ol><li>Cache results locally to eliminate traffic</li><li>Enable complex transfers with single simple request</li><li>Buffer writes in write-back cache on client side</li><li>Pre-fetch large reads into local cache</li></ol><li>Bottlenecks can be avoided by:</li><ol><li>Avoiding single control points (partition responsibility over nodes)</li><li>Separated data- and control-planes</li><ol><li>Control nodes just choreograph data flow</li><li>Data nodes are in charge of committing data transfer and talking to client once client request is redirected to them by control node.</li><li>Dynamic repartitioning of responsibilities</li></ol></ol><li>Scalability of performance can be improved by following the following:</li><ol><li>Reduce amount of consensus operations by electing master nodes before hand. Set up responsibilities before the nodes start running</li><li>Avoid large consensus groups</li><li>Avoid high communication fan-in/fan-out and prefer hierarchical, well-defined info gathering/distribution.</li></ol><li>The minimum interface of a device includes:</li><ol><li>status register: conveys device status</li><li>command register: for issuing commands to device</li><li>data: for passing data to device</li></ol><li>Interrupts come with implied overhead of context switching and interrupt handling. Therefore, interrupt callback implementations are only worth if they are associated with long-lasting informations and the callback will not be called any time soon.</li><li>Anticipatory disk scheduling implies that write scheduling is performed by the OS itself. i.e. instead of immediately sending write requests to disk, the OS will buffer writes and choose upon what it thinks is the best option to send to disk.</li><li>RAID performance metrics include:</li><ol><li>Performance</li><li>Reliability</li><li>Capacity</li></ol><li>RAID-0 just provides parallelism through use of multiple disks by writing adjacent blocks on separate disks.</li><li>RAID-1 is same as RAID-0 except that half of the disks are used for mirroring in order to improve reliability.</li><li>RAID-4 provides reliability through a single parity disk.</li><li>Additive parity implies that upon a block write, all adjacent disk values are read and parity reevaluated before writing to parity disk. Subtractive parity implies that we compare the new bits with the old one’s. If the bits match, we do not touch the parity disk.</li><li>Small random writes are a big problem with RAID-4. It requires the needle of the parity disk to jump around a lot and is bottlenecked by this disk.</li><li>RAID-5 is similar to RAID-4 except that it addresses the random small write problem of RAID-4 by spreading parity blocks across many disks instead of putting all parity data on one disk.</li><li>Preallocation implies that a file is given many adjacent blocks upon creation to improve performance (even though the file might not need them).</li><li>RAM is dynamically partitioned between the virtual memory pages of all processes and the in-memory file system pages. Both are part of the unified page cache and the amount of memory each one receives is allocated dynamically.</li><li>Object storage has more flexibility in terms of metadata type and amount. The metadata can capture application-specific/user-specific info for better indexing. Provides mechanisms for centralizing storage management over vast amounts of data. on dedicated metadata node.</li><li>FAT is characterized by a file allocation table that keeps track of which blocks have been allocated to which files.</li><li>The DOS FAT boot record contains:</li><ol><li>branch instruction: pointer to bootstrap code</li><li>BIOS Parameter Block: the volume descriptor (superblock analog)</li><li>bootstrap code: Code run upon boot to set up the file FAT file system</li><li>FDISK table: Contains disk partition data</li><li>Signature: A corruption checksum</li></ol><li>MicroSoft would use two identical copies of the File Allocation Table.</li><li>FUSE - File System in User Space. Software interface for UNIX OS that lets non-privileged users create file systems without kernel mediation.</li><li>FFS tries to provide a disk-geometry aware file system implementation:</li><ol><li>want to reduce seek frequency and amount</li><li>divide data into groups</li><li>write a directory and all its children inside single group</li></ol><li>Very large files are distributed among groups in order not to mess up spacial locality for other files that share the same parent directory as the large files. Namely, only the first 12 blocks of a large file are written to the usual group. Then, the next 12 blocks are written in another (preferably free) group.</li><li>Small sized files may cause a lot of internal fragmentation (since each file will be given one block by default). So, two possible solutions:</li><ol><li>Use sub-blocks: Divide if necessary one block into subblocks. Once file becomes bigger than on block size, use normal blocks to represent it. Issue: it is hard now to write to single sub block as that would require reading in all other sub blocks in block and re-writing the composite whole block to disk.</li><li>Use a buffer: cache small files into RAM and don’t commit them onto disk until they become at least one block large.</li></ol><li>To make sure the journal is left in a consistent state after crash, we will write the TxE block only once we know all other blocks of that log entry have been written. This implies that as long as the TxE is not written the associated transaction will possibly be lost in the disk.</li><li>The process of writing to disk while ensuring consistency in journaling file systems includes the following steps:</li><ol><li>Data write: Write data blocks to their final destination.</li><li>Journal metadata write: Write the metadata to log. (TxB, I[v2], V[v2])</li><li>Journal commit: Write the transaction commit block (TxE)</li><li>Checkpoint metadata: Write contents of log’s metadata to their final locations</li><li>Free: Mark the log entry in journal as free.</li></ol><li>Back pointer based consistency implies that every pointed to data structure has a back pointer to the data that points to it. These pointers must agree in order for consistency to be proven.</li><li>Log Based File Systems need to employee sophisticated garbage collection mechanisms to recycle no-more-valid data in the log.</li><li>The recursive update problem names the issue of updating stale pointers in LFS. For example, updating a file’s block requires the inode data to change to point to the new data block, so we need to write the inode block too. Now, the inode is in a new spot, so whoever points to the inode needs to update its pointers, and so on… The issue is addressed through the map — which forces the inode numbers to stay constant even if the inode location changes.</li><li>The segment summary block is used to help GC. The segment summary block sits in front of a bunch of blocks. Each entry in the segment summary block maps a block to the inode location that points to it. If the inode location in the segment summary block has been overwritten and is no longer valid, we know the block is stale (i.e. if inode instance is stale, the block is stale).</li><li>Latent Sector Errors: easily detectable disk errors. Combated by duplicating data stored in them on some other part of disk in order not to loose it.</li><li>Disk corruption is detected via checksums.</li><li>To handle misdirected writes we need to store the desired write location in the block header. We can later check if the block header actually matches the location of the physical write.</li><li>The following are the most basic goals of security in systems:</li><ol><li>Integrity</li><li>Availability</li><li>Confidentiality</li></ol><li>Eight design principles of secure systems:</li><ol><li>Acceptability</li><li>Least privilege</li><li>Least common mechanism (e.g. each process has its own page table so can’t play with other page tables)</li><li>open design</li><li>fail safe defaults</li><li>complete mediation</li><li>separation of privilege (e.g. two-factor authentication: Double credentials for critical operations)</li><li>economy of mechanism</li></ol><li>Seven fallacies of distributed systems:</li><ol><li>L - 0 latency</li><li>I - infinite bandwidth of network</li><li>T - Topology of nodes does not change</li><li>A - Only one administrator</li><li>N - network is reliable</li><li>T - cost of transfer between nodes is 0</li><li>S - system is secure</li></ol><li>Three main challenges of distributed systems:</li><ol><li>Performance</li><li>Security</li><li>Handling failure</li></ol><li>We can ensure secure transfer by two means:</li><ol><li>Assign unique ID to each message (cumbersome data structures needed)</li><li>Have counter in both processes. (much easier and just as reliable)</li></ol><li>Distributed file systems provide communication with multiple server nodes, while remote file systems require that we talk to only a single storage base (one node). Distributed file systems give us the advantage of data stripping which decreases latency significantly. Also, the distributed solution is more scalable. However, it is also more complicated.</li><li>RESTful interfaces exhibit:</li><ol><li>U - Uniform Interface</li><li>C - Cacheability</li><li>L - Layered system</li><li>C - Client/server model</li><li>S - Stateless</li></ol><li>Three phase commit consensus for binary consensus case:</li><ol><li>Coordinator asks cohorts “would you be OK with me committing this value?”. If some say no, then abort. This is usually done through sending ack s from coordinator and waiting for ack s from all cohorts.</li><li>If all cohorts respond with ack s, the coordinator must send a precommit message to all cohorts. This way, the cohorts know that the coordinator is still awake. If the coordinator fails to send this message, the cohorts abort the transaction.</li><li>The coordinator commits the message.</li></ol><li>AFS addresses cache inconsistency by issuing callbacks to all clients that have a cached file that is updated on the server. The directory traversal overhead is mitigated by caching all traversals on the server cache.</li><li><span>flushes it to the server, and before the server can issue the callback it crashes, other clients will continue using the old version of that file. To deal with this, when a server recovers from a crash, it sends a message to all clients notifying them about the crash telling them to treat all their cached contents as suspect. Then, just like in the previous case, the client issues TestAuth on all files it has cached to check their validity.<br/></span><br/>The problem with that is if a server takes a long time to recover from a crash, clients can perform a large number of tasks with the old (possibly stale) versions of their files. To deal with this, the server-client often employ a heartbeat message that periodically tells each client that the server is still alive. When the clients don’t receive the ‘heartbeat’ from the server, they stop doing any more work to avoid using old versions of files.</li><li dir="ltr">ACID stands for Atomicity, Consistency, Isolation, Durability.</li><li dir="ltr">Multi versioning avoids reader and writer lock dependencies by satisfying read requests by responding with the version of the file of the last time the write lock was last released on the file.</li></ol><div><br/></div></body></html>